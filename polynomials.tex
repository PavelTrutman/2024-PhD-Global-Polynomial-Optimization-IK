\chapter{Polynomial optimization methods}\labelcha{pol}
In this chapter, we briefly review the methods for polynomial systems solving and for solving polynomial optimization problems.

First, the state of the art method for solving systems of polynomial equations over complex numbers will be reviewed.
This method is based on Gr\"obner bases~\cite{Becker93} computation.
When a Gr\"obner basis is found then the solutions are obtained by computing eigenvalues and eigenvectors of so-called multiplication matrix.
Secondly, we review method based on moments which can be used for solving systems of polynomial equations over real numbers and for solving polynomial optimization problems, i.e.\ optimizing a polynomial function with constraints given as polynomial equations and inequalities.
This method is based on solving hierarchies of semidefinite problems.

We introduce the minimal possible amount of notation for algebraic geometry.
The overall review of the key concepts can be seen in~\cite{Cox-Little-Shea2015}.
We will also follow the notation from~\cite{Cox-Little-Shea2015}.

\section{The method of multiplication matrices}
This method is widely used to find all complex solutions of a system of polynomials \refeqb{pol:mm:polynomialSystem}, where $x\in\C^n$ and $f_1, f_2, \ldots, f_m\in\C[x]$.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rl}
    f_1(x) &= 0\\
    f_2(x) &= 0\\
    &\vdots\\
    f_m(x) &= 0
  \end{array}\labeleq{pol:mm:polynomialSystem}
\end{align}
Let us denote the ideal generated by the polynomials $f_1, f_2, \ldots, f_m$ by $\Ideal = \langle f_1, f_2, \ldots, f_m\rangle$. The set of all complex solutions of the system \refeqb{pol:mm:polynomialSystem} is called the algebraic variety and we denote it by $V_\C$.

The method consists of three phases:
\begin{enumerate}
  \item A Gr\"obner basis $G$ of the ideal $\Ideal$ is found.
  \item Based on the Gr\"obner basis $G$, the multiplication matrix $\MM_f$ is constructed.
  \item If the ideal $\Ideal$ is zero-dimensional (i.e.\ the polynomial system has a finite number of complex solutions), solutions of the system \refeqb{pol:mm:polynomialSystem} are obtained as eigenvectors of the matrix $\MM_f$.
\end{enumerate}

Since the most difficult task is to find a Gr\"obner basis of the ideal $\Ideal$, we will review the state of the art algorithms for computing Gr\"obner bases in the following section.

\subsection{Algorithms for Gr\"obner bases computation}\labelsec{pol:mm:algs}
The very first algorithm~\cite{Buchberger} for computing Gr\"obner bases was introduced by Bruno Buchberger in 1965.
This algorithm is based on the observation that we can extend the set of original equations $F = \{f_1, f_2, \ldots, f_m\}$ to a Gr\"obner basis by adding more polynomials from $\Ideal$.
A natural choice what to add to $F$ are the S-polynomials~\cite{Becker93} reduced by $F$.
The algorithm is as follows:
\begin{enumerate}
  \item Select a pair of polynomials $f_i$ and $f_j$ from $F$ and compute the S-polynomial of them.
  \item Reduce the S-polynomial by the set $F$.
  \item If the remainder of the S-polynomial is non-zero, add it to the set $F$.
  \item Continue by step 1 by selecting a new pair of polynomials.
\end{enumerate}
The algorithm terminates when all possible pairs of polynomials have been considered. Then the set $F$ is a Gr\"obner basis of the ideal \Ideal.

The main disadvantage of this na\"ive approach is that all of the pairs have to be considered even if many of them lead to S-polynomials that will reduce do zero, and therefore will add nothing to the set $F$.
Also, the constructed Gr\"obner bases by this algorithm are often bigger than necessary.

This algorithm can be improved by considering the first and second Buchberger's criteria~\cite{Becker93}.
These two criteria can significantly reduce the number of polynomial pairs to consider.
Also, some superfluous polynomials can be removed from $F$ at the point of adding a new polynomial to $F$.
Smaller set $F$ also significantly reduces the number of all pairs that have to be considered.
This update to the Buchberger algorithm has been done by Gebauer and M\"oller~\cite{Gebauer-Moller88}.

Another significant improvement to the original Buchberger algorithm has been done by Jean-Charles Faug\`ere.
His \FFFF{} algorithm~\cite{F4} replaces the classical polynomial reduction found in the Buchberger algorithm by a simultaneous reduction of several polynomials, which is achieved by symbolic precomputation.
The \FFFF{} algorithm speeds up the reduction step by implementing the division of multiple polynomials as Gauss-Jordan (G-J) elimination of a single matrix.
The speedup is even more significant because sparse linear algebra methods can be used for the G-J elimination step.

The last improvement to the algorithm for Gr\"obner bases computation we mention here is the \FFFFF{} algorithm~\cite{F5} also by Jean-Charles Faug\`ere.
The idea of the algorithm is that with the use of syzygies~\cite{Cox-Little-Shea2015} we can recognize the pairs of polynomials, which S-polynomials will reduce to zero, in advance.
In this way, we can remove all reductions to zero.
However, the \FFFFF{} algorithm considers only principal syzygies, and therefore some reductions to zero may still occur.

An implementation of the \FFFF{} algorithm has been done by J.-C. Faug\`ere himself and it is called FGb~\cite{fgb} (for Fast Gb).
FGb is implemented in C but it has also an interface to Maple.
Moreover, recent versions of Maple are even shipped with FGb together.

\subsection{Automatic generators}\labelsec{pol:autogen}
The algorithms described in the previous section can be used in general for computation of Gr\"obner bases of any set of generators $F$.
But in the typical application, we do not need a general algorithm.
What we need is an algorithm that can solve our problem fast, efficiently and precisely.
And in most cases, the general algorithm will not be the fastest one, since it can be easily beaten by some specialized algorithm, which has been fine-tuned for the given application.

For example, we would like to solve a large number of instances of some structured system of polynomial equations, which vary just in few parameters between all instances of the problem, but the structure of the polynomials is fixed and known in advance.
We can use a general algorithm, which will solve each of the instances from scratch.
Or we can trace the general algorithm on one instance of the problem, cut out the parts which led to dead ends and then we can use this simplified version of the algorithm on all other instances of the problem.
This so-called solver of the problem can speed up the computation in orders of magnitude w.r.t.\ the general algorithm.

In the past, specialized solvers have been handcrafted ad hoc for concrete problems.
But these solvers were difficult to transfer and reuse for even similar problems.
Therefore the process of creating specialized solvers for a given problem was automated by automatic generators~\cite{AutoGen, Larsson2017}.
These automatic generators are based on Gr\"obner bases computation and the latter one by Larrson et al.\ is using even reduction based on syzygies.

These automatic generators are mainly used to generate solvers for so-called minimal problems (e.g.~\cite{p3p, 5pt}) from the geometry of computer vision.
These minimal problems are typically solved in the RANSAC framework~\cite{ransac}, where they are solved repeatedly for a large number of inputs, and therefore the solvers are required to be fast.

\section{Lasserre's hierarchies}\labelsec{pol:lass}
Until now, we have presented a method for computing all complex solutions of a system of polynomial equations with a finite number of solutions.
Now, we will present the method of Lasserre's hierarchies~\cite{Lasserre} which can be used for solving systems of polynomial equations over real numbers.
Moreover, with this method, we can also incorporate polynomial inequalities and we can even optimize a polynomial function on the set of all solutions.
To sum it up, this method allows us to solve any polynomial optimization problem (POP), i.e.\ to optimize a polynomial function on a feasible set constrained by polynomial equations and polynomial inequalities.

This allows us to solve the same problems that can be solved by Gr\"obner bases and multiplication matrices computation (i.e.\ systems of polynomials equations with a finite number of complex solutions) with the advantage that this method will find only real solutions of the problem, which is often desired for problems that come from the geometry.
Moreover, we can even solve underconstrained systems of polynomial equations by adding a polynomial objective function to optimize.
This method even certifies that the found optimum is global.
Some systems that come from the application may be even overconstrained.
Such systems have a solution when solved on precise data using precise arithmetic, but they have no solution when solved on real noisy data.
Fortunately, these systems can be transferred into optimization problems by relaxing the constraints and by minimizing the error of these constraints.

We can see that this method is more general and allow us to solve some problems, which we are unable to solve by the algebraic geometry approach.

To be as general as possible, let us define a general polynomial optimization problem (POP) for polynomials $f, g_1, g_2, \ldots, g_k, h_1, h_2, \ldots, h_l\in\R[x]$:
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p^* &=& \displaystyle \min_{x\in\R^n} & \multicolumn{3}{l}{f(x)} \\
    && \text{s.t.} & g_i(x) &\geq& 0 & (i = 1,\ldots,k) \\
    &&& h_j(x) &=& 0 & (j = 1,\ldots,l)
  \end{array}\labeleq{pol:lass:pop}
\end{align}

In general, the problem~\refeqb{pol:lass:pop} is non-convex and can have several local minima and several global minima.
We are, of course, interested in global minima only.
The minimum $p^*$ can be attained at several points $x^*\in\R^n$.

\subsection{Solving POP as a semidefinite program}
It has been shown by J.~B.~Lasserre~\cite{Lasserre} that the original polynomial optimization problem~\refeqb{pol:lass:pop} can be rewritten into equivalent following semidefinite program~\refeqb{pol:lass:sdp} by introducing a new variable for each monomial of the original polynomial optimization problem~\refeqb{pol:lass:pop}.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p^* &=& \displaystyle \inf_{y\in\R^{\N^n}} & \multicolumn{3}{l}{\displaystyle \sum_{\alpha\in\N^n}{f}_\alpha y_\alpha} \\
    && \text{s.t.} & y_{0\ldots0} &=& 1\\
    &&& M(y) &\succeq& 0\\
    &&& M(g_iy) &\succeq& 0 & (i = 1,\ldots,k) \\
    &&& \sum_{\alpha\in\N^n}h_\alpha y_\alpha &=& 0 & \forall h \in \langle h_1, h_2, \ldots h_l \rangle
  \end{array}\labeleq{pol:lass:sdp}
\end{align}
By \N{} we mean all natural numbers including zero.
By $f_\alpha$ we denote the coefficient of $f$ at the monomial $x^\alpha$ of any polynomial $f\in\R[x]$ and any $n$-tuple $\alpha\in\N^n$.
Notation $M\succeq0$ means that the matrix $M$ is semidefinite positive.

Notation $M(y)$ denotes a moment matrix~\cite[page 53]{SOS}.
The moment matrix $M(y)$ has a form
\begin{align}
  M(y)^{(\alpha,\beta)} &= y_{\alpha+\beta},
\end{align}
where $\alpha, \beta\in\N^n$ and where $M^{(\alpha,\beta)}$ means an element of $M$ at row $\alpha$ and column $\beta$.

Notation $M(fy)$ denotes a localizing matrix~\cite[page 53]{SOS}.
The localizing matrix $M(fy)$ for polynomial $f\in\R[x]$ has a form
\begin{align}
  M(fy)^{(\alpha,\beta)} &= \sum_{\gamma\in\N^n}f_\gamma y_{\alpha+\beta+\gamma},
\end{align}
where $\alpha, \beta\in\N^n$.

Since the optimization problem~\refeqb{pol:lass:sdp} is a semidefinite program, its objective function is linear and the constraints are linear equations and linear matrix inequalities (LMI).
Therefore, the optimization problem~\refeqb{pol:lass:sdp} has a convex objective function and a convex feasible set.
Unfortunately, this problem is infinite-dimensional, and therefore is not solvable by computers.
Moreover, the last constraint of~\refeqb{pol:lass:sdp} represents an infinite number of linear equations.

\subsection{Solving POP by Lasserre's LMI hierarchies}
To overcome the problem with infinite dimensions of the semidefinite program~\refeqb{pol:lass:sdp}, we relax the problem by limiting the degree of considered monomials.
Consider the Lasserre's LMI hierarchy~\refeqb{pol:lass:sdpr}, where $r\in\N$ is the relaxation order.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p_r^* &=& \displaystyle \inf_{y\in\R^{\N^n_{2r}}} & \multicolumn{3}{l}{\displaystyle \sum_{\alpha\in\N^n_{2r}}{f}_\alpha y_\alpha} \\
    && \text{s.t.} & y_{0\ldots0} &=& 1\\
    &&& M_r(y) &\succeq& 0\\
    &&& M_{r-r_i}(g_iy) &\succeq& 0 & (i = 1,\ldots,k) \\
    &&& \sum_{\alpha\in\N^n_{2r}}h_\alpha y_\alpha &=& 0 & \forall h \in \{h_jx^\alpha\ |\ j = 1,\ldots,l, |\alpha|\leq 2r-\deg(h_j)\}
  \end{array}\labeleq{pol:lass:sdpr}
\end{align}
By $\N_d$ we mean all natural numbers including zero up to number $d\in\N$.
Where $r_i = \left\lceil\frac{\deg(g_i)}{2}\right\rceil$ and $r \geq r_{min}$ for
\begin{align}
  r_{min} = \max\left\{\left\lceil\frac{\deg(f)}{2}\right\rceil, \left\lceil\frac{\deg(g_i)}{2}\right\rceil, \left\lceil\frac{\deg(h_j)}{2}\right\rceil\ |\ i = 1,\ldots, k, j = 1, \ldots, l\right\}.
\end{align}

Notation $M_s(y)$ denotes a truncated moment matrix~\cite[page 53]{SOS}.
The truncated moment matrix $M_s(y)$ has a form
\begin{align}
  M_s(y)^{(\alpha,\beta)} &= y_{\alpha+\beta},
\end{align}
where $\alpha, \beta\in\N^n_s$.

Notation $M_s(fy)$ denotes a truncated localizing matrix~\cite[page 53]{SOS}.
The truncated localizing matrix $M_s(fy)$ for polynomial $f\in\R[x]$ has a form
\begin{align}
  M_s(fy)^{(\alpha,\beta)} &= \sum_{\gamma\in\N^n}f_\gamma y_{\alpha+\beta+\gamma},
\end{align}
where $\alpha, \beta\in\N^n_s$.

The semidefinite program~\refeqb{pol:lass:sdpr} is a relaxation of the program~\refeqb{pol:lass:sdp} and even of the original polynomial optimization problem~\refeqb{pol:lass:pop}.
The semidefinite program~\refeqb{pol:lass:sdpr} is now finite and convex and can be therefore easily solved by gradient descent methods.

\begin{theorem}[Lasserre's LMI hierarchy converges {\thecite[Proposition~3.3]{HenrionLectures}}]\labelthe{pol:lass:hierConv}
  For $r\in\N$ there holds
  \begin{align}
    p^*_r \leq p^*_{r+1} \leq p^*
  \end{align}
  and
  \begin{align}
    \lim_{r\rightarrow+\infty}p^*_r &= p^*.
  \end{align}
\end{theorem}

By \refthe{pol:lass:hierConv} we know that with increasing relaxation order $r$, the minimum $p_r^*$ of the relaxed semidefinite program~\refeqb{pol:lass:sdpr} gives us tighter and tighter lower bound of the global minimum of the original polynomial problem~\refeqb{pol:lass:pop}.
Moreover, for $r\rightarrow+\infty$ the relaxed semidefinite program becomes the infinite-dimensional semidefinite program~\refeqb{pol:lass:sdp}, and therefore $p_r^*$ reaches the optimal value $p^*$.
Fortunately, the next theorem says that typically $p_r^*$ reaches the optimal value $p^*$ even for a small value of $r$.

\begin{theorem}[Generic finite convergence {\thecite[Proposition~3.4]{HenrionLectures}}]\labelthe{pol:lass:convergence}
  In the finite-dimensional space of coefficients of the polynomials $g_i$ and $h_j$, $i = 1, \ldots, k$ and $j = 1, \ldots, l$, defining the problem~\refeqb{pol:lass:pop}, there is a low-dimensional algebraic set, which is such that if we choose an instance of the problem~\refeqb{pol:lass:pop} outside of this set, the Lasserre's LMI relaxations have finite convergence, i.e.\ there exists a finite $r^*\in\N$ such that $p^*_r=p^*$ for all $r\in\N\colon r\geq r^*$.
\end{theorem}

As a result of \refthe{pol:lass:convergence}, we know that in general, it is enough to compute one relaxed semidefinite program~\refeqb{pol:lass:sdpr}.
If the chosen relaxation order is big enough then by solving this relaxed program we obtain the global minimum of the original polynomial optimization problem~\refeqb{pol:lass:pop}.
The theorem also says that only for some degenerate problems which are rare the finite convergence does not occur and the global minimum can not be obtained by solving relaxed semidefinite program~\refeqb{pol:lass:sdpr} for any finite relaxation order $r$.

\begin{theorem}[Certificate of finite convergence {\thecite[Proposition~3.5]{HenrionLectures}}]\labelthe{pol:lass:certConv}
  Let $y^*$ be the solution of the problem~\refeqb{pol:lass:sdpr} at a given relaxation order $r \geq r_{min}$.
  If
  \begin{align}
    \rank M_{r-r_{min}}(y^*) &= \rank M_r(y^*), \labeleq{pol:lass:certConv}
  \end{align}
  then $p^*_r = p^*$.
\end{theorem}

Because of \refthe{pol:lass:convergence}, we know that finite convergence of Lasserre's LMI hierarchies occurs, but it is not known to us at which relaxation order $r$ it happens.
This information provides us \refthe{pol:lass:certConv}.
The typical approach is to start with the minimal possible relaxation order $r_{min}$ and by verifying \refeq{pol:lass:certConv} we know if we have reached the global minimum.
If not, we go higher in the Lasserre's hierarchy by incrementing $r$ by one until \refthe{pol:lass:certConv} is satisfied.

When \refthe{pol:lass:certConv} is satisfied and we know that we have reached the global minimum, values $x^*$ of the original polynomial problem~\refeqb{pol:lass:pop} at which the global minimum is attained can be recovered from the solution $y^*$ of the relaxed semidefinite program~\refeqb{pol:lass:sdpr}.

This method of Lasserre's LMI hierarchies is implemented as a toolbox for MATLAB which is named GloptiPoly~\cite{gloptipoly}.
The main advantage of this toolbox is that it uses tool YALMIP~\cite{yalmip} as a unified interlayer to communicate with many different semidefinite solvers, e.g.\ SeDuMi~\cite{sedumi} or MOSEK~\cite{mosek} can be used.
Another polynomial optimization solver based on Lasserre's hierarchies is GpoSolver~\cite{gposolver} which generates \CPP{} code based on MATLAB description of the problem.
The generated \CPP{} code can then solve the given problem for any parameters.
Worth mentioning is also the SOSTOOLS~\cite{sostools} toolbox for MATLAB, which is able to solve the sum of squares optimization problems.
